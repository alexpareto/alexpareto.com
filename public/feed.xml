<feed xmlns="http://www.w3.org/2005/Atom">
<generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator>
<link href="https://alexpareto.com/feed.xml" rel="self" type="application/atom+xml"/>
<link href="https://alexpareto.com/" rel="alternate" type="text/html"/>
<updated>2023-12-12T05:16:25+00:00</updated>
<id>https://alexpareto.com/feed.xml</id>
<title type="html">Alex Pareto</title>
<subtitle>Hey! My name's Alex. I work on engineering at NTWRK. I'm based in LA. I write here mostly about software and startups. Previously I founded Demeanor (Y Combinator S18) which was acquired by NTWRK in 2019. Before that, I worked on the video platform at Facebook. I studied at USC and Andover.</subtitle>
<entry>
<title type="html">Choosing a Tech Stack</title>
<link href="https://alexpareto.com/2020/09/13/choosing-tech-stack.html" rel="alternate" type="text/html" title="Choosing a Tech Stack"/>
<published>2020-09-13T00:00:00+00:00</published>
<updated>2020-09-13T00:00:00+00:00</updated>
<id>https://alexpareto.com/2020/09/13/choosing-tech-stack</id>
<content type="html" xml:base="https://alexpareto.com/2020/09/13/choosing-tech-stack.html"><p>The best tech stack for a startup is the one that lets our team be the most productive. We get a lot of things with productivity: we can ship more, find product market fit faster, avoid running out of runway, and be happier.</p> <p>So what’s the framework for choosing a tech stack that we can be productive with? After spending some time thinking about the common themes in past choices, I came up with a framework to use when making these decisions. It’s shared below (and is still evolving).</p> <h2 id="the-framework">The Framework</h2> <p>First we need to figure out two things:</p> <ol> <li>Who will be using this tech stack?</li> <li>What are we building?</li> </ol> <p>These two question are important because they shape the four guidelines to follow when looking for a tech stack:</p> <ol> <li>Use what we already know</li> <li>Don’t reinvent the basics</li> <li>Choose something that scales for performance and for hiring</li> <li>Follow the leaders</li> </ol> <p>Let’s dive into these in a bit more detail.</p> <h3 id="use-what-we-already-know">Use What We Already Know</h3> <p>This is the first trait because it’s likely the most important. Spending time learning new technologies can be a huge slow down on productivity early on in a project. Even when we do get up to speed, it can take years to understand all the nuances of a new language or technology. If we already know the technology, we will debug problems faster, find solutions more quickly, and be more productive.</p> <p>This is why knowing who will use the tech stack is important. If the whole team is extremely proficient at Angular and nobody knows React, then the team will build an Angular MVP much faster than a React one.</p> <h3 id="dont-reinvent-the-basics">Don’t Reinvent the Basics</h3> <p>“Basics” is intentionally vague. This looks very different for various projects. If a “basic” tool in our application is concurrency, then Go may be a better choice than Python. On the other hand if a basic primitive in our application is lots of CRUD endpoints, perhaps using a framework like Rails or Django is a better choice.</p> <p>In general, it is possible to implement these “basics” in most languages (if we use what we know), but this time could also be spent talking to customers or getting closer to product/market fit. The best stack lets us not think about the basics and focus on building what makes our application unique instead.</p> <h3 id="choose-something-that-scales-for-performance-and-for-hiring">Choose Something That Scales for Performance and for Hiring</h3> <p>This becomes important after we hit product market fit and start to grow the company. The less we have to think about optimizing the performance of our application, the more differentiating features we can ship.</p> <p>Thinking about hiring is also important. If we choose an obscure language or technology - for example, a Graph DB over a relational one - we will have a lot harder time recruiting and hiring. I’ve found the <a href="https://insights.stackoverflow.com/survey/2020#most-loved-dreaded-and-wanted">most wanted and most loved lists</a> on Stack Overflow for languages and frameworks to be a good measure of this.</p> <h3 id="follow-the-leaders">Follow the Leaders</h3> <p>Following the leaders means that the stack is in use by other companies in the industry. I like to look to other startups in the space that have successfully scaled and were founded relatively recently. Having a tech stack supported by other industry veterans means that we know some things out of the box:</p> <ol> <li>It will scale for hiring and performance (it’s already been done)</li> <li>There will be a large community for support and help</li> </ol> <p><a href="https://stackshare.io/">StackShare</a> is a good source to find information about what technologies are in use by various startups and larger companies. The other place would be companies’ careers pages (see what technology they look for in hiring).</p> <hr /> <p>I’d love to hear thoughts on this, especially if others have useful guidelines that are missing. Shoot me an email (hey@alexpareto.com) or find me on Twitter @alexpareto.</p></content>
<author>
<name/>
</author>
<summary type="html">The best tech stack for a startup is the one that lets our team be the most productive. We get a lot of things with productivity: we can ship more, find product market fit faster, avoid running out of runway, and be happier.</summary>
</entry>
<entry>
<title type="html">Dealing With Spiky Traffic and Thundering Herds</title>
<link href="https://alexpareto.com/2020/06/15/thundering-herds.html" rel="alternate" type="text/html" title="Dealing With Spiky Traffic and Thundering Herds"/>
<published>2020-06-15T00:00:00+00:00</published>
<updated>2020-06-15T00:00:00+00:00</updated>
<id>https://alexpareto.com/2020/06/15/thundering-herds</id>
<content type="html" xml:base="https://alexpareto.com/2020/06/15/thundering-herds.html"><p><img src="/assets/spiky_traffic.png" alt="/assets/spiky_traffic.png" /></p> <p>The graph above is a concurrent user count screenshot taken over the course of a few minutes. We can look at it in a couple ways. The first is excitement: more users! This feeling usually lasts until the infrastructure comes crashing down to reality. At that point the feeling of excitement might be replaced with panic. How do we keep the servers from collapsing if this keeps happening?</p> <p>Dealing with a 10-100x increase in users is hard - especially if you run an app where spikes are a frequent occurrence. With the advent of push notifications and live streaming these traffic patterns are becoming more and more common. This is a thundering herd: users stampede your services - often taking down everything in their path.</p> <p>The first thing that goes down is usually the database. Thousands of queries hit the database at once and trigger a massive slow down. It’s not uncommon for the database CPU graph to look just like the graph above. When the database slows down, requests start timing out. Timeouts and long requests overload the application servers. Soon everything is falling over. Not good.</p> <p>We need to stop the herd (or at least slow it down). This brings us to step one:</p> <h2 id="cache-liberally">Cache Liberally</h2> <p>When load on the database is spiking from a slow or frequently used query, the easiest way to solve the issue is to throw the data in a cache. We can use a tool like Redis for this - or take it one step further and cache the data on the edge using a CDN like Cloudflare or Cloudfront.</p> <p>The advantage of a CDN is that the request will never hit our service. Even when traffic increases by 100x our service will hum along as if nothing changed.</p> <p>For many services, using a cache will be enough. But for others, the thundering herd results in so many concurrent users that when the cache expires, we end up with the same problem all over again.</p> <p><img src="/assets/thundering_herd.gif" alt="/assets/thundering_herd.gif" /></p> <p>Since there are so many concurrent users, for every cache miss that occurs, all requests will be let through during the time that the cache value is being recalculated. The underlying service will collapse in response to this.</p> <p>So how do we stop all these requests from coming through?</p> <h2 id="we-need-to-close-the-gate">We Need to Close the Gate</h2> <p>Clearly our underlying server cannot handle this much traffic. We need to “close the gate” and not let the requests in. Of course, this alone won’t work. We still need to serve something!</p> <p>The key insight here is that every request is asking for the same data. Instead of letting them all through the gate, let’s only allow <em>one</em> request through to recalculate the data. After all, the data will be the same for every request.</p> <p>Now, what do we do with all the other requests?</p> <h3 id="option-1-serve-the-others-stale-data">Option 1: Serve the Others Stale Data</h3> <p>Serving stale data has the advantage of being relatively simple and fast. The idea here is that we serve the old data in the cache while we’re recalculating the current up-to-date data.</p> <p>Using a cache like Redis or Memcached, this would look something like the following:</p> <ol> <li>Store a key in the cache with a fake, longer than desired expire time.</li> <li>In this key, store both the data and the <em>real</em> cache expire time.</li> <li>On fetching the key, check the <em>real</em> expire time. If it has expired: <ol> <li>Update the real expire key to be in the future (this will prevent other requests from attempting to recalculate the data).</li> <li>Recalculate the data and then update the key with the latest data, so all subsequent requests will get the latest data.</li> </ol> </li> </ol> <p>If possible, the read and update steps should be done in an atomic way. This will prevent any other request from recalculating the data at the same time.</p> <h3 id="option-2-have-the-others-wait-for-the-latest-data">Option 2: Have the Others Wait for the Latest Data</h3> <p>We can also have the other requests wait for the latest data from the request. This is called request coalescing or request collapsing. Many CDN providers have this functionality built in (often in advanced settings).</p> <p><img src="/assets/coalesced_herd.gif" alt="/assets/coalesced_herd.gif" /></p> <p>Using something like Redis or Memcached to solve this problem would look like the following:</p> <ol> <li>Attempt to get a key from the cache.</li> <li>If key is not there, then attempt to acquire a lock on the data with the Redis key as the lock name (one option here is to use a distributed lock with multiple servers).</li> <li>If the lock is acquired, then recalculate the data and refill the cache.</li> <li>If the lock is not acquired, wait for the lock to be available again and check if the cache has been filled - if it has, another request has already done the recalculation! Return that data.</li> </ol> <p>A queue is another popular way to tackle this problem. See the resources below for ideas there.</p> <h2 id="conclusion">Conclusion</h2> <p>And with that, the requests are satisfied! Next time a massive spike in traffic occurs, our servers will have enough breathing room from the cache to safely scale up and our database will live to see another day.</p> <p>That is, until we have to deal with data that can’t be cached globally… perhaps that’s a problem for another day.</p> <h2 id="resources">Resources</h2> <p>Source for GIFs on this Page: <a href="https://www.facebook.com/watch/?v=10153675295382200">Video from Facebook Engineering on Thundering Herds</a></p> <p><a href="https://issues.apache.org/jira/browse/TS-3549">Apache Traffic Server Ticket for Handling Thundering Herds</a></p> <p><a href="http://highscalability.com/strategy-break-memcache-dog-pile">Break up the Memcache Dogpile on High Scalability</a></p> <p><a href="https://docs.trafficserver.apache.org/en/latest/admin-guide/plugins/collapsed_forwarding.en.html">Apache Traffic Server Collapsed Forwarding</a></p> <p><a href="https://docs.fastly.com/en/guides/request-collapsing">Fastly Request Collapsing</a></p></content>
<author>
<name/>
</author>
<summary type="html"/>
</entry>
<entry>
<title type="html">Scaling to 100k Users</title>
<link href="https://alexpareto.com/scalability/systems/2020/02/03/scaling-100k.html" rel="alternate" type="text/html" title="Scaling to 100k Users"/>
<published>2020-02-03T00:00:00+00:00</published>
<updated>2020-02-03T00:00:00+00:00</updated>
<id>https://alexpareto.com/scalability/systems/2020/02/03/scaling-100k</id>
<content type="html" xml:base="https://alexpareto.com/scalability/systems/2020/02/03/scaling-100k.html"><p>Many startups have been there - what feels like legions of new users are signing up for accounts every day and the engineering team is scrambling to keep things running.</p> <p>It’s a good a problem to have, but information on how to take a web app from 0 to hundreds of thousands of users can be scarce. Usually solutions come from either massive fires popping up or by identifying bottlenecks (and often times both).</p> <p>With that said, I’ve noticed that many of the main patterns for taking a side project to something highly scalable are relatively formulaic.</p> <p>This is an attempt to distill the basics around that formula into writing. We’re going to take our new photo sharing website, Graminsta, from 1 to 100k users.</p> <h2 id="1-user-1-machine">1 User: 1 Machine</h2> <p>Nearly every application, be it a website or a mobile app - has three key components: an API, a database, and a client (usually an app or a website). The database stores persistent data. The API serves requests for and around that data. The client renders that data to the user.</p> <p>I’ve found in modern application development that thinking of the client as a completely separate entity from the API makes it much easier to reason about scaling the application.</p> <p>When we first start building the application, it’s alright for all three of these things to run on one server. In a way, this resembles our development environment: one engineer runs the database, API, and client all on the same computer.</p> <p>In theory, we could deploy this to the cloud on a single DigitalOcean Droplet or AWS EC2 instance like below:</p> <p><img src="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-01-21_at_8.11.27_AM.png" alt="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-01-21_at_8.11.27_AM.png" /></p> <p>With that said, if we expect Graminsta to be used by more than 1 person, it almost always makes sense to split out the database layer.</p> <h2 id="10-users-split-out-the-database-layer">10 Users: Split out the Database Layer</h2> <p>Splitting out the database into a managed service like Amazon’s RDS or Digital Ocean’s Managed Database will serve us well for a long time. It’s slightly more expensive than self-hosting on a single machine or EC2 instance - but with these services you get a lot of easy add ons out of the box that will come in handy down the line: multi-region redundancy, read replicas, automated backups, and more.</p> <p>Here’s what the Graminsta system looks like now:</p> <p><img src="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-01-21_at_8.13.17_AM.png" alt="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-01-21_at_8.13.17_AM.png" /></p> <h2 id="100-users-split-out-the-clients">100 Users: Split Out the Clients</h2> <p>Lucky for us, our first few users love Graminsta. Now that traffic is starting to get more steady, it’s time to split out the client. One thing to note is that <strong>splitting out</strong> entities is a key aspect of building a scalable application. As one part of the system gets more traffic, we can split it out so that we can handle scaling the service based on its own specific traffic patterns.</p> <p>This is why I like to think of the client as separate from the API. It makes it very easy to reason about building for multiple platforms: web, mobile web, iOS, Android, desktop apps, third party services etc. They’re all just clients consuming the same API.</p> <p>In the same vein, the biggest feedback we’re getting from users is that they want Graminsta on their phones. So we’re going to launch a mobile app while we’re at it.</p> <p>Here’s what the system looks like now:</p> <p><img src="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-02-03_at_9.56.51_PM.png" alt="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-02-03_at_9.56.51_PM.png" /></p> <h2 id="1000-users-add-a-load-balancer">1,000 Users: Add a Load Balancer.</h2> <p>Things are picking up at Graminsta. Users are uploading photos left and right. We’re starting to get more sign ups. Our lonely API instance is having trouble keeping up with all the traffic. We need more compute power!</p> <p>Load balancers are very powerful. The key idea is that we place a load balancer in front of the API and it will route traffic to an instance of that service. This allows for horizontal scaling (increasing the amount of requests we can handle by adding more servers running the same code).</p> <p>We’re going to place a separate load balancer in front of our web client and our API. This means we can have multiple instances running our API and web client code. The load balancer will route requests to whichever instance has the least traffic.</p> <p>What we also get out of this is redundancy. When one instance goes down (maybe it gets overloaded or crashes), then we have other instances still up to respond to incoming requests - instead of the whole system going down.</p> <p>A load balancer also enables autoscaling. We can set up our load balancer to increase the number of instances during the Superbowl when everyone is online and decrease the number of instances when all of our users are asleep.</p> <p>With a load balancer, our API layer can scale to practically infinity, we will just keep adding instances as we get more requests.</p> <p><img src="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-01-21_at_8.25.50_AM.png" alt="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-01-21_at_8.25.50_AM.png" /></p> <blockquote> <p>Side note: At this point what we have so far is very similar to what PaaS companies like Heroku or AWS’s Elastic Beanstalk provide out of the box (and why they’re so popular). Heroku puts the database on a separate host, manages a load balancer with autoscaling, and lets you host your web client separately from your API. This is a great reason to use a service like Heroku for projects or early stage startups - all of the necessary basics come out of the box.</p> </blockquote> <h2 id="10000-users-cdn">10,000 Users: CDN</h2> <p>We probably should have done this from the beginning, but we’re moving fast here at Graminsta. Serving and uploading all these images is starting to put way too much load on our servers.</p> <p>We should be using a cloud storage service to host static content at this point: think images, videos, and more (AWS’s S3 or Digital Ocean’s Spaces). In general, our API should avoid handling things like serving images and image uploads.</p> <p>The other thing we get out of a cloud storage service is a CDN (in AWS this is an add-on called Cloudfront, but many cloud storage services offer it out of the box). A CDN will automatically cache our images at different data centers throughout the world.</p> <p>While our main data center may be hosted in Ohio, if someone requests an image from Japan, our cloud provider will make a copy and store it at their data center in Japan. The next person to request that image in Japan will then recieve it much faster. This is important when we need to serve larger file sizes like images or videos that take a long time to load + send across the world.</p> <p><img src="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-01-21_at_8.30.06_AM.png" alt="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-01-21_at_8.30.06_AM.png" /></p> <h2 id="100000-users-scaling-the-data-layer">100,000 Users: Scaling the Data Layer</h2> <p>The CDN helped us out a lot - things are booming at Graminsta. A YouTube celebrity, Mavid Mobrick, just signed up and posted us on their story. The API CPU and memory usage is low across the board - thanks to our load balancer adding 10 API instances to the environment - but we’re starting to get a lot of timeouts on requests…why is everything taking so long?</p> <p>After some digging we see it: the Database CPU is hovering at 80-90%. We’re maxed out.</p> <p>Scaling the data layer is probably the trickiest part of the equation. While for API servers serving stateless requests, we can merely add more instances, the same is not true with <em>most</em> database systems. In this case, we’re going to explore the popular relational database systems (PostgreSQL, MySQL, etc.).</p> <h3 id="caching">Caching</h3> <p>One of the easiest ways to get more out of our database is by introducing a new component to the system: the cache layer. The most common way to implement a cache is by using an in-memory key value store like Redis or Memcached. Most clouds have a managed version of these services: Elasticache on AWS and Memorystore on Google Cloud.</p> <p>The cache comes in handy when the service is making lots of repeated calls to the database for the same information. Essentially we hit the database once, save the information in the cache, and never have to touch the database again.</p> <p>For example, in Graminsta every time someone goes to Mavid Mobrick’s profile page, the API layer requests Mavid Mobrick’s profile information from the database. This is happening over and over again. Since Mavid Mobrick’s profile information isn’t changing on every request, that info is a great candidate to cache.</p> <p>We’ll cache the result from the database in Redis under the key <code class="highlighter-rouge">user:id</code> with an expiration time of 30 seconds. Now when someone goes to Mavid Mobrick’s profile, we check Redis first and just serve the data straight out of Redis if it exists. Despite Mavid Mobrick being the most popular on the site, requesting the profile puts hardly any load on our database.</p> <p>The other plus of most cache services, is that we can scale them out easier than a database. Redis has a built in Redis Cluster mode that, in a similar way to a load balancer<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, lets us distribute our Redis cache across multiple machines (thousands if one so pleases).</p> <p>Nearly all highly scaled applications take ample advantage of caching, it’s an absolutely integral part of making a fast API. Better queries and more performant code are all a part of the equation, but without a cache none of these will be sufficient to scale to millions of users.</p> <h3 id="read-replicas">Read Replicas</h3> <p>The other thing we can do now that our database has started to get hit quite a bit, is to add read replicas using our database management system. With the managed services above, this can be done in one-click. A read replica will stay up to date with your master DB, and will be able to be used for SELECT statements.</p> <p>Here’s our system now:</p> <p><img src="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-01-21_at_8.35.01_AM.png" alt="/assets/Scaling%20to%20100k%20Users/Screen_Shot_2020-01-21_at_8.35.01_AM.png" /></p> <h2 id="beyond">Beyond</h2> <p>As the app continues to scale, we’re going to want to focus on splitting out services that can be scaled independently. For example, if we start to make use of websockets, it would make sense to pull out our websocket handling code. We can put this on new instaces behind their own load balancer that can scale up and down based on how many websocket connections have been opened or closed, independently of how many HTTP requests we have coming in.</p> <p>We’re also going to continue to bump up against limitations on the data layer. This is when we are going to want to start looking into partitioning and sharding the database. These both require more overhead, but effectively allow the data layer to scale infinitely.</p> <p>We will want to make sure that we have monitoring installed using a service like New Relic or Datadog. This will ensure we understand what requests are slow and where improvement needs to be made. As we scale we will want to be focused on finding bottlenecks and fixing them - often by taking advantage of some of the ideas in the previous sections.</p> <p>Hopefully at this point, we have some other people on the team to help out as well!</p> <h4 id="references">References:</h4> <p>This post was inspired by one of <a href="http://highscalability.com/blog/2016/1/11/a-beginners-guide-to-scaling-to-11-million-users-on-amazons.html">my favorite posts on High Scalability</a>. I wanted to flesh the article out a bit more for the early stages and make it a bit more cloud agnostic. Definitely check it out if you’re interested in these kind of things.</p> <h4 id="translations">Translations</h4> <p>Readers have kindly translated this post into <a href="https://www.infoq.cn/article/Tyx5HwaD9OKNX4xzFaFo">Chinese</a>, <a href="https://leonkim.dev/systems/scaling-100k/">Korean</a>, and <a href="https://www.ibidemgroup.com/edu/salto-a-100000-usuarios/">Spanish</a>.</p> <h4 id="footnotes">Footnotes</h4> <div class="footnotes"> <ol> <li id="fn:1"> <p>While similar in terms of allowing us to spread load across multiple instances, the underlying implementation of Redis Cluster is much different than a load balancer. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p> </li> </ol> </div></content>
<author>
<name/>
</author>
<summary type="html">Many startups have been there - what feels like legions of new users are signing up for accounts every day and the engineering team is scrambling to keep things running.</summary>
</entry>
</feed>